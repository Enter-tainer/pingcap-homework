# top-100-url

## Idea

1. 读取整个文件, 按 hash 结果将 url append 到不同的文件里面, 保证相同url分布在相同的文件里面
2. 依次处理每一个分片, 统计每个分片里面的前100名 (usize, string), 
    - 具体操作大概是整一个hashmap, 存一下出现次数
    - 然后弄一个大小是100的堆, 把hashmap过一遍.(似乎也可以把哈希表里面的东西都摊平放进数组里面, 然后跑快速选择,是O(n)的, 但是内存占用可能会不够用). 
3. 在处理分片的过程中, 统计总的前100名, 最终获得总的前 100 名.(似乎还是可以用堆搞, 维护一个小顶堆, 和堆顶比较一下就可). 

如果单个分片过大应该如何处理?

1. 处理分片时应当边读边统计, 一次读入一大块然后塞进哈希表, 然后再继续读入是不错的选择. 哈希表的大小不会超过分片本身的大小.

2. 似乎可以直接用 BufReader, with_capacity (read_buffer_size).

如何保证 1G 的内存限制?

1. 预处理阶段, 峰值内存占用大小约为 read_buffer_size.
2. 处理分片时, 内存占用大小约为 read_buffer_size + hashmap_size, 如果使用了快速选择, 内存占用会增加到 vector_size, 大概和哈希表的大小差不多.
3. hashmap_size 大小不定, 与分片中本质不同的 URL 数量有关(O(n)).
4. 处理分片时, 最坏情况为: 分片中每个 url 都不同, 此时, 内存占用大小会约等于分片大小.
5. 不妨假设不存在毒瘤情况.. 常规意义下的访问log应该遵循2-8定律.

总结: 分片大小应当约等于内存限制, 或比内存限制稍小一点, 是比较合理的
